{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import glob\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df, column_name): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[column_name]\n",
    "        if column_name == 'Intensity Class':   # If oc then need to alter valence representation\n",
    "            valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quiri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 records have no words left after text cleaning in 2018-EI-oc-En-joy-test.txt\n",
      "1 records have no words left after text cleaning in 2018-EI-oc-En-sadness-dev.txt\n",
      "1 records have no words left after text cleaning in 2018-EI-oc-En-sadness-test.txt\n",
      "1 records have no words left after text cleaning in 2018-EI-reg-En-joy-test.txt\n",
      "1 records have no words left after text cleaning in 2018-EI-reg-En-sadness-dev.txt\n",
      "1 records have no words left after text cleaning in 2018-EI-reg-En-sadness-test.txt\n",
      "1 records have no words left after text cleaning in 2018-Valence-oc-En-test.txt\n",
      "1 records have no words left after text cleaning in 2018-Valence-reg-En-test.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data/raw/'\n",
    "out_folder_path = 'data/cleaned/'\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "raw_filepaths = glob.glob(folder_path + '*')\n",
    "# Loop over raw files\n",
    "for filepath in raw_filepaths:\n",
    "    basename = os.path.basename(filepath)\n",
    "    basename_ = basename.split('-') # Get list of splitted basename for use later\n",
    "    if basename_[-1][-4:] != '.txt':\n",
    "        print('Did not clean', filepath + '. Item did not end in .txt.')\n",
    "        continue\n",
    "    # Read in df\n",
    "    df = pd.read_table(filepath)\n",
    "    \n",
    "    # Get valence score\n",
    "    if basename_[2] == 'oc':\n",
    "        val_col = \"Intensity Class\"\n",
    "    elif basename_[2] == 'reg':\n",
    "        val_col = \"Intensity Score\"\n",
    "    else:\n",
    "        print('Classification type not found. basename_[2]', basename_[2])\n",
    "    valence_list = adapt_valence_scores(df, val_col)\n",
    "    \n",
    "    \n",
    "    # If data is English\n",
    "    if basename_[3] == 'En':\n",
    "        ct = CleanText()\n",
    "        clean_ = ct.fit_transform(df.Tweet)\n",
    "\n",
    "    # If data is Arabic\n",
    "    elif basename_[3] == 'Ar':\n",
    "        continue\n",
    "        \n",
    "    # Fill empty cells with '[no_text]'\n",
    "    empty_clean = clean_ == ''\n",
    "    if clean_[empty_clean].count() > 0:\n",
    "        print(f'{clean_[empty_clean].count()} records have no words left after text cleaning in {basename}')\n",
    "        clean_.loc[empty_clean] = '[no_text]'\n",
    "        \n",
    "    # Create new df of cleaned data\n",
    "    df_cleaned = pd.DataFrame(clean_)\n",
    "    # Add valence score to list\n",
    "    df_cleaned['Valence score'] = valence_list\n",
    "        \n",
    "    # Drop rows with no tweet text\n",
    "    df_cleaned = df_cleaned[df_cleaned['Tweet'] != '[no_text]']\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Write to csv\n",
    "    if len(basename_) == 5:\n",
    "        outpath = out_folder_path + '-'.join(['cleaned', basename_[1], basename_[2], basename_[3], basename_[-1]])\n",
    "    elif len(basename_) == 6:\n",
    "        outpath = out_folder_path + '-'.join(['cleaned', basename_[1], basename_[2], basename_[3], basename_[4], basename_[-1]])\n",
    "    else:\n",
    "        print('something went wrong with basename length. It is not 5 or 6 elements long.')\n",
    "    # For example \"data/cleaned/cleaned_Valence_oc_En_train.txt'\n",
    "    df_cleaned.to_csv(outpath, sep=\"\\t\")\n",
    "\n",
    "#df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
