{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"2018-Valence-oc-En-train.txt\"\n",
    "dev_path = \"2018-Valence-oc-En-dev.txt\"\n",
    "test_path = \"2018-Valence-oc-En-test-gold.txt\"\n",
    "\n",
    "training_outpath = \"cleaned_training.txt\"\n",
    "dev_outpath = \"cleaned_dev.txt\"\n",
    "test_outpath = \"cleaned_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-En-01964</td>\n",
       "      <td>Gm and have a  #Tuesday!</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-En-01539</td>\n",
       "      <td>@realDonaldTrump But you have a lot of time for tweeting #ironic</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-En-04235</td>\n",
       "      <td>I graduated yesterday and already had 8 family members asking what job I've got now ðŸ˜‚ #nightmare</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-En-03711</td>\n",
       "      <td>@jaimitoelcrack7 Seriously...I've been sitting here for five minutes watching this in awe. It never gets less amazing.</td>\n",
       "      <td>valence</td>\n",
       "      <td>1: slightly positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-En-01177</td>\n",
       "      <td>Whether my glass is half empty or its half full. I'm just grateful I even have a glass and that there's something in it.\\n #optimism ðŸ¤”</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2018-En-04059</td>\n",
       "      <td>Premier League Teams should fear next seasons Arsenal's XI. #coyg #afc</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2018-En-01488</td>\n",
       "      <td>how are you my love? @Hashtag_DonJon love youu!! thanks for the smile, that motivates me to keep going!! ðŸ’› so blessed to have you ðŸ˜Š</td>\n",
       "      <td>valence</td>\n",
       "      <td>3: very positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2018-En-02648</td>\n",
       "      <td>'She is the clothed with strength and dignity, and she laughs without fear of the future.' ðŸ’›ðŸŒ¿ @jessconte</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2018-En-03444</td>\n",
       "      <td>My dads big day is only less than 2 weeks away. ðŸ˜± #excited</td>\n",
       "      <td>valence</td>\n",
       "      <td>3: very positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2018-En-04040</td>\n",
       "      <td>And let the depression take the stage once more ðŸ™ƒ</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  \\\n",
       "0    2018-En-01964   \n",
       "1    2018-En-01539   \n",
       "2    2018-En-04235   \n",
       "3    2018-En-03711   \n",
       "4    2018-En-01177   \n",
       "..             ...   \n",
       "932  2018-En-04059   \n",
       "933  2018-En-01488   \n",
       "934  2018-En-02648   \n",
       "935  2018-En-03444   \n",
       "936  2018-En-04040   \n",
       "\n",
       "                                                                                                                                      Tweet  \\\n",
       "0    Gm and have a  #Tuesday!                                                                                                                 \n",
       "1    @realDonaldTrump But you have a lot of time for tweeting #ironic                                                                         \n",
       "2    I graduated yesterday and already had 8 family members asking what job I've got now ðŸ˜‚ #nightmare                                         \n",
       "3    @jaimitoelcrack7 Seriously...I've been sitting here for five minutes watching this in awe. It never gets less amazing.                   \n",
       "4    Whether my glass is half empty or its half full. I'm just grateful I even have a glass and that there's something in it.\\n #optimism ðŸ¤”   \n",
       "..                                                                                                                                      ...   \n",
       "932  Premier League Teams should fear next seasons Arsenal's XI. #coyg #afc                                                                   \n",
       "933  how are you my love? @Hashtag_DonJon love youu!! thanks for the smile, that motivates me to keep going!! ðŸ’› so blessed to have you ðŸ˜Š      \n",
       "934  'She is the clothed with strength and dignity, and she laughs without fear of the future.' ðŸ’›ðŸŒ¿ @jessconte                                 \n",
       "935  My dads big day is only less than 2 weeks away. ðŸ˜± #excited                                                                               \n",
       "936  And let the depression take the stage once more ðŸ™ƒ                                                                                        \n",
       "\n",
       "    Affect Dimension                                         Intensity Class  \n",
       "0    valence          0: neutral or mixed emotional state can be inferred     \n",
       "1    valence          0: neutral or mixed emotional state can be inferred     \n",
       "2    valence          0: neutral or mixed emotional state can be inferred     \n",
       "3    valence          1: slightly positive emotional state can be inferred    \n",
       "4    valence          2: moderately positive emotional state can be inferred  \n",
       "..       ...                                                             ...  \n",
       "932  valence          0: neutral or mixed emotional state can be inferred     \n",
       "933  valence          3: very positive emotional state can be inferred        \n",
       "934  valence          0: neutral or mixed emotional state can be inferred     \n",
       "935  valence          3: very positive emotional state can be inferred        \n",
       "936  valence          -3: very negative emotional state can be inferred       \n",
       "\n",
       "[937 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_table(training_path)\n",
    "dev_df = pd.read_table(dev_path)\n",
    "test_df = pd.read_table(test_path)\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[\"Intensity Class\"]\n",
    "        valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list\n",
    "        \n",
    "training_valence = adapt_valence_scores(training_df)\n",
    "dev_valence = adapt_valence_scores(dev_df)\n",
    "test_valence = adapt_valence_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "training_clean = ct.fit_transform(training_df.Tweet)\n",
    "dev_clean = ct.fit_transform(dev_df.Tweet)\n",
    "test_clean = ct.fit_transform(test_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.DataFrame(training_clean)\n",
    "training_cleaned['Valence score']= training_valence\n",
    "\n",
    "dev_cleaned = pd.DataFrame(dev_clean)\n",
    "dev_cleaned['Valence score']= dev_valence\n",
    "\n",
    "test_cleaned = pd.DataFrame(test_clean)\n",
    "test_cleaned['Valence score']= test_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.to_csv(training_outpath, sep=\"\\t\")\n",
    "dev_cleaned.to_csv(dev_outpath, sep=\"\\t\")\n",
    "test_cleaned.to_csv(test_outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([271], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-090a2a92fb2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m271\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m271\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1837\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m             return self.obj._reindex_with_indexers(\n\u001b[0;32m   1135\u001b[0m                 \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 raise KeyError(\n\u001b[0;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1177\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m                     )\n\u001b[0;32m   1179\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([271], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df.loc[[271]]\n",
    "test_cleaned.loc[[271]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
