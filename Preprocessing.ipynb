{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import glob\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#from snowballstemmer import stemmer\n",
    "#arabic_stemmer = stemmer(\"arabic\")\n",
    "\n",
    "stopwords_english = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df, column_name): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[column_name]\n",
    "        if column_name == 'Intensity Class':   # If oc then need to alter valence representation\n",
    "            valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"utilities/arabic-stop-words-list.txt\", \"r\", encoding=\"utf-8\") as infile: \n",
    "    stopwords = list()\n",
    "    for line in infile:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText_English(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_repeating_char(self, input_text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', input_text) #keep 2 repeat\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_repeating_char).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText_Arabic(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_repeating_char(self, input_text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', input_text) #keep 2 repeat\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r\"http\\S+ | www\\S+\" , \"لينك\", input_text)\n",
    "    \n",
    "    def remove_hashtags(self, input_text):\n",
    "        return re.sub(r\"#\", \"\", input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        ### whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords) and len(word) > 1] \n",
    "        return \" \".join(clean_words)\n",
    "    \n",
    "    #def stem(self, input_text):\n",
    "        #words = input_text.split()\n",
    "        #stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "        #return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_hashtags).apply(self.remove_repeating_char).apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/raw/'\n",
    "out_folder_path = 'data/cleaned/'\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "# Get all files in folder to be cleaned\n",
    "raw_filepaths = glob.glob(folder_path + '*')\n",
    "# Loop over raw files\n",
    "for filepath in raw_filepaths:\n",
    "    # Get basename\n",
    "    basename = os.path.basename(filepath)\n",
    "    basename_ = basename.split('-') # Get list of splitted basename \n",
    "    # If a file is not a txt file, then print the file path, and skip to next\n",
    "    if basename_[-1][-4:] != '.txt':\n",
    "        print('Did not clean', filepath + '. Item did not end in .txt.')\n",
    "        continue\n",
    "    # Read in df\n",
    "    df = pd.read_table(filepath)\n",
    "    \n",
    "    # Get valence score (column name is different for type of classification)\n",
    "    if basename_[2] == 'oc':\n",
    "        val_col = \"Intensity Class\"\n",
    "    elif basename_[2] == 'reg':\n",
    "        val_col = \"Intensity Score\"\n",
    "    else:\n",
    "        print('Classification type not found. basename_[2]', basename_[2])\n",
    "    # Get valence list from df\n",
    "    valence_list = adapt_valence_scores(df, val_col)\n",
    "    \n",
    "    \n",
    "    # If data is English\n",
    "    if basename_[3] == 'En':\n",
    "        continue\n",
    "        ct = CleanText_English()\n",
    "        clean_ = ct.fit_transform(df.Tweet)\n",
    "\n",
    "    # If data is Arabic\n",
    "    elif basename_[3] == 'Ar':\n",
    "        ct = CleanText_Arabic()\n",
    "        clean_ = ct.fit_transform(df.Tweet)\n",
    "        \n",
    "    # Fill empty cells with '[no_text]', and print how many there are\n",
    "    empty_clean = clean_ == ''\n",
    "    if clean_[empty_clean].count() > 0:\n",
    "        print(f'{clean_[empty_clean].count()} records have no words left after text cleaning in {basename}')\n",
    "        clean_.loc[empty_clean] = '[no_text]'\n",
    "        \n",
    "    # Create new df of cleaned data\n",
    "    df_cleaned = pd.DataFrame(clean_)\n",
    "    # Add valence score to list\n",
    "    df_cleaned['Valence score'] = valence_list\n",
    "        \n",
    "    # Drop rows with no tweet text\n",
    "    df_cleaned = df_cleaned[df_cleaned['Tweet'] != '[no_text]']\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Write to csv\n",
    "    if len(basename_) == 5:\n",
    "        outpath = out_folder_path + '-'.join(['cleaned', basename_[1], basename_[2], basename_[3], basename_[-1]])\n",
    "    elif len(basename_) == 6:\n",
    "        outpath = out_folder_path + '-'.join(['cleaned', basename_[1], basename_[2], basename_[3], basename_[4], basename_[-1]])\n",
    "    else:\n",
    "        print('something went wrong with basename length. It is not 5 or 6 elements long.')\n",
    "    # For example \"data/cleaned/cleaned_Valence_oc_En_train.txt'\n",
    "    df_cleaned.to_csv(outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
