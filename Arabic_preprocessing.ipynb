{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Moved CleanText_Arabic function to preprocessing.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from snowballstemmer import stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"data/raw/2018-Valence-oc-Ar-train.txt\"\n",
    "dev_path = \"data/raw/2018-Valence-oc-Ar-dev.txt\"\n",
    "test_path = \"data/raw/2018-Valence-oc-Ar-test.txt\"\n",
    "\n",
    "training_outpath = \"data/cleaned/Ar_cleaned_training.txt\"\n",
    "dev_outpath = \"data/cleaned/Ar_cleaned_dev.txt\"\n",
    "test_outpath = \"data/cleaned/Ar_cleaned_test.txt\"\n",
    "\n",
    "stopwords_path = \"utilities/arabic-stop-words-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-Ar-01961</td>\n",
       "      <td>Ø¥Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ø£Ù„Ù… ÙŠØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø­</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-Ar-03289</td>\n",
       "      <td>@Al3mriRami @Holyliviuss ÙƒÙ„ Ù…Ø§ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ø£Ù†ÙŠ ØºØ§Ø¶...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-Ar-04349</td>\n",
       "      <td>ÙŠØ­Ø°Ø±ÙƒÙ… ÙˆÙŠØ®ÙˆÙÙƒÙ… Ù…Ù† Ù†ÙØ³Ù‡ Ø§Ø°Ø§ Ø§Ø±ØªÙƒØ¨ØªÙ… Ø°Ù†Ø¨ Ø§Ùˆ Ù…Ø¹ØµÙŠ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-Ar-03640</td>\n",
       "      <td>ğŸ’ ğŸ’ ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ Ø§Ù„Ù„Ù‡ ØµÙŠ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>3: very positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-Ar-01176</td>\n",
       "      <td>@sjalmulla Ø´ÙØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø³Ø¨ÙˆØ¹ ÙˆÙ…ØªØ´ÙˆÙ‚Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ§ÙŠØ¯ Ø§Ù„...</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2018-Ar-01836</td>\n",
       "      <td>@GafnZx ğŸ˜³\\n\\nÙ‡Ø°Ø§ Ø¨Ø§Ù„Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù†Øª  Ø§Ù„Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰</td>\n",
       "      <td>valence</td>\n",
       "      <td>1: slightly positive emotional state can be in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2018-Ar-03475</td>\n",
       "      <td>Ø§Ù„ØµØ§Ø¯Ù… ÙÙŠ Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø§Ù…Ø± Ø§Ù† Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹Øª Ù…Ø´Ø§ÙƒØ³Ø© Ù…Ø«Ù„ ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2018-Ar-01490</td>\n",
       "      <td>ÙƒÙ„ ØªÙ„Ùƒ Ø§Ù„Ø£ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø­Ù…Ù„ Ø±Ø£Ø³Ùƒ Ù„ÙƒÙ† Ùˆ Ù„ Ø³ÙˆØ¡ ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-1: slightly negative emotional state can be i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2018-Ar-01710</td>\n",
       "      <td>Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠØª Ø¨Ø·Ø¹Ù… ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2018-Ar-02662</td>\n",
       "      <td>Ø¨ØªØ®Ø§Ù Ø¹ Ø§Ø®ÙˆØ§ØªÙ‡Ø§ Ø§ÙˆÙ‰ Ø¯Ø§ÙŠÙ…Ø§ Ø³Ø± Ø§Ø®ÙˆÙ‡Ø§ Ø¨ØªÙƒÙˆÙ† ÙØ±Ø­Ø© ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>932 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  \\\n",
       "0    2018-Ar-01961                     Ø¥Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ø£Ù„Ù… ÙŠØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø­   \n",
       "1    2018-Ar-03289  @Al3mriRami @Holyliviuss ÙƒÙ„ Ù…Ø§ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ø£Ù†ÙŠ ØºØ§Ø¶...   \n",
       "2    2018-Ar-04349  ÙŠØ­Ø°Ø±ÙƒÙ… ÙˆÙŠØ®ÙˆÙÙƒÙ… Ù…Ù† Ù†ÙØ³Ù‡ Ø§Ø°Ø§ Ø§Ø±ØªÙƒØ¨ØªÙ… Ø°Ù†Ø¨ Ø§Ùˆ Ù…Ø¹ØµÙŠ...   \n",
       "3    2018-Ar-03640  ğŸ’ ğŸ’ ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ Ø§Ù„Ù„Ù‡ ØµÙŠ...   \n",
       "4    2018-Ar-01176  @sjalmulla Ø´ÙØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø³Ø¨ÙˆØ¹ ÙˆÙ…ØªØ´ÙˆÙ‚Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ§ÙŠØ¯ Ø§Ù„...   \n",
       "..             ...                                                ...   \n",
       "927  2018-Ar-01836         @GafnZx ğŸ˜³\\n\\nÙ‡Ø°Ø§ Ø¨Ø§Ù„Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù†Øª  Ø§Ù„Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰   \n",
       "928  2018-Ar-03475  Ø§Ù„ØµØ§Ø¯Ù… ÙÙŠ Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø§Ù…Ø± Ø§Ù† Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹Øª Ù…Ø´Ø§ÙƒØ³Ø© Ù…Ø«Ù„ ...   \n",
       "929  2018-Ar-01490  ÙƒÙ„ ØªÙ„Ùƒ Ø§Ù„Ø£ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø­Ù…Ù„ Ø±Ø£Ø³Ùƒ Ù„ÙƒÙ† Ùˆ Ù„ Ø³ÙˆØ¡ ...   \n",
       "930  2018-Ar-01710  Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠØª Ø¨Ø·Ø¹Ù… ...   \n",
       "931  2018-Ar-02662  Ø¨ØªØ®Ø§Ù Ø¹ Ø§Ø®ÙˆØ§ØªÙ‡Ø§ Ø§ÙˆÙ‰ Ø¯Ø§ÙŠÙ…Ø§ Ø³Ø± Ø§Ø®ÙˆÙ‡Ø§ Ø¨ØªÙƒÙˆÙ† ÙØ±Ø­Ø© ...   \n",
       "\n",
       "    Affect Dimension                                    Intensity Class  \n",
       "0            valence  -3: very negative emotional state can be inferred  \n",
       "1            valence  -2: moderately negative emotional state can be...  \n",
       "2            valence  -2: moderately negative emotional state can be...  \n",
       "3            valence   3: very positive emotional state can be inferred  \n",
       "4            valence  2: moderately positive emotional state can be ...  \n",
       "..               ...                                                ...  \n",
       "927          valence  1: slightly positive emotional state can be in...  \n",
       "928          valence  -2: moderately negative emotional state can be...  \n",
       "929          valence  -1: slightly negative emotional state can be i...  \n",
       "930          valence  -2: moderately negative emotional state can be...  \n",
       "931          valence  0: neutral or mixed emotional state can be inf...  \n",
       "\n",
       "[932 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_table(training_path)\n",
    "dev_df = pd.read_table(dev_path)\n",
    "test_df = pd.read_table(test_path)\n",
    "\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path, \"r\", encoding=\"utf-8\") as infile: \n",
    "    stopwords = list()\n",
    "    for line in infile:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[\"Intensity Class\"]\n",
    "        valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list\n",
    "        \n",
    "training_valence = adapt_valence_scores(training_df)\n",
    "dev_valence = adapt_valence_scores(dev_df)\n",
    "test_valence = adapt_valence_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText_Arabic(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_repeating_char(self, input_text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', input_text) #keep 2 repeat\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r\"http\\S+ | www\\S+\" , \"Ù„ÙŠÙ†Ùƒ\", input_text)\n",
    "    \n",
    "    def remove_hashtags(self, input_text):\n",
    "        return re.sub(r\"#\", \"\", input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        ### whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords) and len(word) > 1] \n",
    "        return \" \".join(clean_words)\n",
    "    \n",
    "    #def stem(self, input_text):\n",
    "        #words = input_text.split()\n",
    "        #stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "        #return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_hashtags).apply(self.remove_repeating_char).apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText_Arabic()\n",
    "training_clean = ct.fit_transform(training_df.Tweet)\n",
    "dev_clean = ct.fit_transform(dev_df.Tweet)\n",
    "test_clean = ct.fit_transform(test_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Ø¥Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ø£Ù„Ù… ÙŠØºÙ„Ø¨ Ø§Ù„ÙØ±Ø­\n",
       "1      Ù…Ø§ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ø£Ù†ÙŠ ØºØ§Ø¶Ø¨ Ø£Ù†Ù†Ø§ Ù†Ø¸Ù‡Ø± Ù†Ù‚Ø¯Ù… Ø£Ø³Ø§Ø·ÙŠØ±Ù†Ø§ Ø¨Ø£...\n",
       "2      ÙŠØ­Ø°Ø±ÙƒÙ… ÙˆÙŠØ®ÙˆÙÙƒÙ… Ø§Ø±ØªÙƒØ¨ØªÙ… Ø°Ù†Ø¨ Ù…Ø¹ØµÙŠÙ‡ Ø³ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§...\n",
       "3               ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ ØµÙŠØ§Ù…Ù†Ø§ ÙˆÙ‚ÙŠØ§Ù…Ù†Ø§\n",
       "4      Ø´ÙØªÙ‡ Ø§Ø³Ø¨ÙˆØ¹ ÙˆÙ…ØªØ´ÙˆÙ‚Ù‡ ÙˆØ§ÙŠØ¯ Ø§Ù„ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ ÙŠØ§...\n",
       "                             ...                        \n",
       "927                        nÙ‡Ø°Ø§ Ø¨Ø§Ù„Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù†Øª Ø§Ù„Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰\n",
       "928    Ø§Ù„ØµØ§Ø¯Ù… Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø§Ù…Ø± Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹Øª Ù…Ø´Ø§ÙƒØ³Ø© ÙˆØªØ¶Ø§ÙŠÙ‚Ù†ÙŠ Ù…...\n",
       "929         Ø§Ù„Ø£ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø±Ø© Ø­Ù…Ù„ Ø±Ø£Ø³Ùƒ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ Ø§Ù„ÙƒØªÙ ØªØ­Ø¨ ÙˆØ¬Ø¹\n",
       "930    Ø¨Ø³Ù… Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠØª Ø¨Ø·Ø¹Ù… Ø§Ù„Ø±Ø¹Ø¨...\n",
       "931    Ø¨ØªØ®Ø§Ù Ø§Ø®ÙˆØ§ØªÙ‡Ø§ Ø§ÙˆÙ‰ Ø¯Ø§ÙŠÙ…Ø§ Ø³Ø± Ø§Ø®ÙˆÙ‡Ø§ Ø¨ØªÙƒÙˆÙ† ÙØ±Ø­Ø© Ø§Ù„...\n",
       "Name: Tweet, Length: 932, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.DataFrame(training_clean)\n",
    "training_cleaned['Valence score']= training_valence\n",
    "\n",
    "dev_cleaned = pd.DataFrame(dev_clean)\n",
    "dev_cleaned['Valence score']= dev_valence\n",
    "\n",
    "test_cleaned = pd.DataFrame(test_clean)\n",
    "test_cleaned['Valence score']= test_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.to_csv(training_outpath, sep=\"\\t\")\n",
    "dev_cleaned.to_csv(dev_outpath, sep=\"\\t\")\n",
    "test_cleaned.to_csv(test_outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
