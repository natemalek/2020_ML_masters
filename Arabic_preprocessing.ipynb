{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from snowballstemmer import stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"vaguely_ML_masters/data/raw/2018-Valence-oc-Ar-train.txt\"\n",
    "dev_path = \"vaguely_ML_masters/data/raw/2018-Valence-oc-Ar-dev.txt\"\n",
    "test_path = \"vaguely_ML_masters/data/raw/2018-Valence-oc-Ar-test.txt\"\n",
    "\n",
    "training_outpath = \"vaguely_ML_masters/data/cleaned/Ar_cleaned_training.txt\"\n",
    "dev_outpath = \"vaguely_ML_masters/data/cleaned/Ar_cleaned_dev.txt\"\n",
    "test_outpath = \"vaguely_ML_masters/data/cleaned/Ar_cleaned_test.txt\"\n",
    "\n",
    "stopwords_path = \"vaguely_ML_masters/utilities/arabic-stop-words-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-Ar-01961</td>\n",
       "      <td>Ø¥Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ø£Ù„Ù… ÙŠØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø­</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-Ar-03289</td>\n",
       "      <td>@Al3mriRami @Holyliviuss ÙƒÙ„ Ù…Ø§ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ø£Ù†ÙŠ ØºØ§Ø¶Ø¨ Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø¸Ù‡Ø± ÙˆÙ„Ø§ Ù†Ù‚Ø¯Ù… Ø£Ø³Ø§Ø·ÙŠØ±Ù†Ø§ Ø¨Ø£Ù†ÙØ³Ù†Ø§\\nØºØ§Ø¶Ø¨ Ø¹Ù„Ù‰ Ø£Ù†ÙØ³Ù†Ø§ ÙˆÙ„ÙŠØ³ Ø§Ù„ØºØ±Ø¨</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-Ar-04349</td>\n",
       "      <td>ÙŠØ­Ø°Ø±ÙƒÙ… ÙˆÙŠØ®ÙˆÙÙƒÙ… Ù…Ù† Ù†ÙØ³Ù‡ Ø§Ø°Ø§ Ø§Ø±ØªÙƒØ¨ØªÙ… Ø°Ù†Ø¨ Ø§Ùˆ Ù…Ø¹ØµÙŠÙ‡ ÙØ§Ù†Ù‡ Ø³ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§Ø¨Ù‡ #ÙƒÙ†ÙˆØ² #Ø¯Ø§Ø±_Ø§Ù„Ø±ÙŠØ§Ù†_Ø§Ù„Ù†Ø³Ø§Ø¦ÙŠÙ‡ ~~</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-Ar-03640</td>\n",
       "      <td>ğŸ’ ğŸ’ ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ Ø§Ù„Ù„Ù‡ ØµÙŠØ§Ù…Ù†Ø§ ÙˆÙ‚ÙŠØ§Ù…Ù†Ø§ ğŸ’ ğŸ’</td>\n",
       "      <td>valence</td>\n",
       "      <td>3: very positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-Ar-01176</td>\n",
       "      <td>@sjalmulla Ø´ÙØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø³Ø¨ÙˆØ¹ ÙˆÙ…ØªØ´ÙˆÙ‚Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ§ÙŠØ¯ Ø§Ù„ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ğŸ˜ Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ â¤ï¸</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2018-Ar-01836</td>\n",
       "      <td>@GafnZx ğŸ˜³\\n\\nÙ‡Ø°Ø§ Ø¨Ø§Ù„Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù†Øª  Ø§Ù„Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰</td>\n",
       "      <td>valence</td>\n",
       "      <td>1: slightly positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2018-Ar-03475</td>\n",
       "      <td>Ø§Ù„ØµØ§Ø¯Ù… ÙÙŠ Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø§Ù…Ø± Ø§Ù† Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹Øª Ù…Ø´Ø§ÙƒØ³Ø© Ù…Ø«Ù„ Ù‚Ø¨Ù„ ÙˆØªØ¶Ø§ÙŠÙ‚Ù†ÙŠ Ù…Ø¬Ø¯Ø¯Ø§ ÙˆÙƒØ£Ù†Ù†Ø§ Ù„Ù… Ù†ØªØ­Ø¯Ø« Ø¨Ù‡Ø¯ÙˆØ¡ Ù‚Ø¨Ù„ Ù‚Ù„ÙŠÙ„</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2018-Ar-01490</td>\n",
       "      <td>ÙƒÙ„ ØªÙ„Ùƒ Ø§Ù„Ø£ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø­Ù…Ù„ Ø±Ø£Ø³Ùƒ Ù„ÙƒÙ† Ùˆ Ù„ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ Ù„ÙŠØ³ Ø¨ÙŠÙ†Ù‡Ù… Ø§Ù„ÙƒØªÙ Ø§Ù„ØªÙŠ ØªØ­Ø¨ #ÙˆØ¬Ø¹</td>\n",
       "      <td>valence</td>\n",
       "      <td>-1: slightly negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2018-Ar-01710</td>\n",
       "      <td>Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠØª Ø¨Ø·Ø¹Ù… Ø§Ù„Ø±Ø¹Ø¨ ØµØ¯Ù‚ Ø§Ù„Ø§ Ù„Ù…Ø§ Ø³ÙˆÙ„ÙØª Ù…Ø¹ÙŠ ÙˆØ­Ø¯Ù‡ Ù†Ø§ÙŠÙ…Ù‡ğŸ˜­</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2018-Ar-02662</td>\n",
       "      <td>Ø¨ØªØ®Ø§Ù Ø¹ Ø§Ø®ÙˆØ§ØªÙ‡Ø§ Ø§ÙˆÙ‰ Ø¯Ø§ÙŠÙ…Ø§ Ø³Ø± Ø§Ø®ÙˆÙ‡Ø§ Ø¨ØªÙƒÙˆÙ† ÙØ±Ø­Ø© Ø§Ù„Ø¨ÙŠØª Ù…Ø¨ÙŠØ¹Ø±ÙØ´ Ù‚ÙŠÙ…ØªÙ‡Ø§ ØºÙŠØ± Ø§Ù„Ù„Ù‰ Ù…Ø®Ù„ÙØ´ Ø¨Ù†Ø§Øª Ùˆï»» Ø§ï»»Ø® Ø§Ù„Ù„Ù‰ Ù…Ù„ÙˆØ´ Ø§Ø®Øª</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inferred</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>932 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  \\\n",
       "0    2018-Ar-01961   \n",
       "1    2018-Ar-03289   \n",
       "2    2018-Ar-04349   \n",
       "3    2018-Ar-03640   \n",
       "4    2018-Ar-01176   \n",
       "..             ...   \n",
       "927  2018-Ar-01836   \n",
       "928  2018-Ar-03475   \n",
       "929  2018-Ar-01490   \n",
       "930  2018-Ar-01710   \n",
       "931  2018-Ar-02662   \n",
       "\n",
       "                                                                                                                  Tweet  \\\n",
       "0    Ø¥Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ø£Ù„Ù… ÙŠØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±Ø­                                                                                       \n",
       "1    @Al3mriRami @Holyliviuss ÙƒÙ„ Ù…Ø§ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ø£Ù†ÙŠ ØºØ§Ø¶Ø¨ Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø¸Ù‡Ø± ÙˆÙ„Ø§ Ù†Ù‚Ø¯Ù… Ø£Ø³Ø§Ø·ÙŠØ±Ù†Ø§ Ø¨Ø£Ù†ÙØ³Ù†Ø§\\nØºØ§Ø¶Ø¨ Ø¹Ù„Ù‰ Ø£Ù†ÙØ³Ù†Ø§ ÙˆÙ„ÙŠØ³ Ø§Ù„ØºØ±Ø¨   \n",
       "2    ÙŠØ­Ø°Ø±ÙƒÙ… ÙˆÙŠØ®ÙˆÙÙƒÙ… Ù…Ù† Ù†ÙØ³Ù‡ Ø§Ø°Ø§ Ø§Ø±ØªÙƒØ¨ØªÙ… Ø°Ù†Ø¨ Ø§Ùˆ Ù…Ø¹ØµÙŠÙ‡ ÙØ§Ù†Ù‡ Ø³ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§Ø¨Ù‡ #ÙƒÙ†ÙˆØ² #Ø¯Ø§Ø±_Ø§Ù„Ø±ÙŠØ§Ù†_Ø§Ù„Ù†Ø³Ø§Ø¦ÙŠÙ‡ ~~                   \n",
       "3    ğŸ’ ğŸ’ ØµØ¨Ø§Ø­ÙƒÙ… Ø³Ø¹Ø§Ø¯Ø© ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ Ø§Ù„Ù„Ù‡ ØµÙŠØ§Ù…Ù†Ø§ ÙˆÙ‚ÙŠØ§Ù…Ù†Ø§ ğŸ’ ğŸ’                                                       \n",
       "4    @sjalmulla Ø´ÙØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø³Ø¨ÙˆØ¹ ÙˆÙ…ØªØ´ÙˆÙ‚Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ§ÙŠØ¯ Ø§Ù„ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ğŸ˜ Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ â¤ï¸                                             \n",
       "..                                                                        ...                                             \n",
       "927  @GafnZx ğŸ˜³\\n\\nÙ‡Ø°Ø§ Ø¨Ø§Ù„Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù†Øª  Ø§Ù„Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰                                                                           \n",
       "928  Ø§Ù„ØµØ§Ø¯Ù… ÙÙŠ Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø§Ù…Ø± Ø§Ù† Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹Øª Ù…Ø´Ø§ÙƒØ³Ø© Ù…Ø«Ù„ Ù‚Ø¨Ù„ ÙˆØªØ¶Ø§ÙŠÙ‚Ù†ÙŠ Ù…Ø¬Ø¯Ø¯Ø§ ÙˆÙƒØ£Ù†Ù†Ø§ Ù„Ù… Ù†ØªØ­Ø¯Ø« Ø¨Ù‡Ø¯ÙˆØ¡ Ù‚Ø¨Ù„ Ù‚Ù„ÙŠÙ„                      \n",
       "929  ÙƒÙ„ ØªÙ„Ùƒ Ø§Ù„Ø£ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø­Ù…Ù„ Ø±Ø£Ø³Ùƒ Ù„ÙƒÙ† Ùˆ Ù„ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ Ù„ÙŠØ³ Ø¨ÙŠÙ†Ù‡Ù… Ø§Ù„ÙƒØªÙ Ø§Ù„ØªÙŠ ØªØ­Ø¨ #ÙˆØ¬Ø¹                                      \n",
       "930  Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠØª Ø¨Ø·Ø¹Ù… Ø§Ù„Ø±Ø¹Ø¨ ØµØ¯Ù‚ Ø§Ù„Ø§ Ù„Ù…Ø§ Ø³ÙˆÙ„ÙØª Ù…Ø¹ÙŠ ÙˆØ­Ø¯Ù‡ Ù†Ø§ÙŠÙ…Ù‡ğŸ˜­                                \n",
       "931  Ø¨ØªØ®Ø§Ù Ø¹ Ø§Ø®ÙˆØ§ØªÙ‡Ø§ Ø§ÙˆÙ‰ Ø¯Ø§ÙŠÙ…Ø§ Ø³Ø± Ø§Ø®ÙˆÙ‡Ø§ Ø¨ØªÙƒÙˆÙ† ÙØ±Ø­Ø© Ø§Ù„Ø¨ÙŠØª Ù…Ø¨ÙŠØ¹Ø±ÙØ´ Ù‚ÙŠÙ…ØªÙ‡Ø§ ØºÙŠØ± Ø§Ù„Ù„Ù‰ Ù…Ø®Ù„ÙØ´ Ø¨Ù†Ø§Øª Ùˆï»» Ø§ï»»Ø® Ø§Ù„Ù„Ù‰ Ù…Ù„ÙˆØ´ Ø§Ø®Øª          \n",
       "\n",
       "    Affect Dimension                                          Intensity Class  \n",
       "0    valence          -3: very negative emotional state can be inferred        \n",
       "1    valence          -2: moderately negative emotional state can be inferred  \n",
       "2    valence          -2: moderately negative emotional state can be inferred  \n",
       "3    valence          3: very positive emotional state can be inferred         \n",
       "4    valence          2: moderately positive emotional state can be inferred   \n",
       "..       ...                                                             ...   \n",
       "927  valence          1: slightly positive emotional state can be inferred     \n",
       "928  valence          -2: moderately negative emotional state can be inferred  \n",
       "929  valence          -1: slightly negative emotional state can be inferred    \n",
       "930  valence          -2: moderately negative emotional state can be inferred  \n",
       "931  valence          0: neutral or mixed emotional state can be inferred      \n",
       "\n",
       "[932 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_table(training_path)\n",
    "dev_df = pd.read_table(dev_path)\n",
    "test_df = pd.read_table(test_path)\n",
    "\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path, \"r\", encoding=\"utf-8\") as infile: \n",
    "    stopwords = list()\n",
    "    for line in infile:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[\"Intensity Class\"]\n",
    "        valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list\n",
    "        \n",
    "training_valence = adapt_valence_scores(training_df)\n",
    "dev_valence = adapt_valence_scores(dev_df)\n",
    "test_valence = adapt_valence_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_repeating_char(self, input_text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', input_text) #keep 2 repeat\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r\"http\\S+ | www\\S+\" , \"Ù„ÙŠÙ†Ùƒ\", input_text)\n",
    "    \n",
    "    def remove_hashtags(self, input_text):\n",
    "        return re.sub(r\"#\", \"\", input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        ### whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords) and len(word) > 1] \n",
    "        return \" \".join(clean_words)\n",
    "    \n",
    "    def stem(self, input_text):\n",
    "        words = input_text.split()\n",
    "        stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_hashtags).apply(self.remove_repeating_char).apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.remove_stopwords).apply(self.stem)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "training_clean = ct.fit_transform(training_df.Tweet)\n",
    "dev_clean = ct.fit_transform(dev_df.Tweet)\n",
    "test_clean = ct.fit_transform(test_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Ø§Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ù… ÙŠØºÙ„Ø¨ ÙØ±Ø­                                                                  \n",
       "1      Ù…Ø§Ù Ø§Ù…Ø± Ø§Ù† ØºØ§Ø¶Ø¨ Ø§Ù†Ù†Ø§ Ù†Ø¸Ù‡Ø± Ù†Ù‚Ø¯Ù… Ø§Ø³Ø§Ø·ÙŠØ± Ø§Ù†ÙØ³ nØºØ§Ø¶Ø¨ Ø§Ù†ÙØ³ ØºØ±Ø¨                               \n",
       "2      ÙŠØ­Ø°Ø± ÙŠØ®ÙˆÙ Ø§Ø±ØªÙƒØ¨ Ø°Ù†Ø¨ Ù…Ø¹Øµ ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§Ø¨ ÙƒÙ†ÙˆØ² Ø¯Ø§Ø±Ø§Ù„Ø±ÙŠØ§Ù†Ø§Ù„Ù†Ø³Ø§Ø¡                              \n",
       "3      ØµØ¨Ø§Ø­ Ø³Ø¹Ø§Ø¯ Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ ØµÙŠØ§Ù… Ù‚ÙŠØ§Ù…                                                          \n",
       "4      Ø´ÙØª Ø§Ø³Ø¨ÙˆØ¹ Ù…ØªØ´ÙˆÙ‚ ÙˆØ§ÙŠØ¯ ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ â¤ï¸                                             \n",
       "                          ...                                                                  \n",
       "927    nÙ‡Ø° Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù† Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰                                                                  \n",
       "928    ØµØ§Ø¯Ù… Ø­Ù‚ÙŠÙ‚ Ø§Ù…Ø± Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹ Ù…Ø´Ø§ÙƒØ³ ØªØ¶Ø§ÙŠÙ‚ Ù…Ø¬Ø¯Ø¯ ÙˆÙƒØ¡ Ù†ØªØ­Ø¯Ø« Ù‡Ø¯ÙˆØ¡ Ù‚Ù„ÙŠÙ„                             \n",
       "929    Ø§ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø± Ø­Ù…Ù„ Ø±Ø§Ø³ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ ÙƒØªÙ ØªØ­Ø¨ ÙˆØ¬Ø¹                                                  \n",
       "930    Ø¨Ø³Ù… Ø±Ø­Ù… Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠ Ø·Ø¹Ù… Ø±Ø¹Ø¨ ØµØ¯Ù‚ Ø³ÙˆÙ„Ù Ù…Ø¹ ÙˆØ­Ø¯ Ù†Ø§ÙŠÙ…Ù‡ğŸ˜­                            \n",
       "931    ØªØ®Ø§Ù Ø§Ø®ÙˆØ§ Ø§ÙˆÙŠ Ø¯Ø§ÙŠÙ… Ø³Ø± Ø§Ø®Ùˆ ØªÙƒÙˆÙ† ÙØ±Ø­ Ø¨ÙŠØª Ù…Ø¨ÙŠØ¹Ø±ÙØ´ Ù‚ÙŠÙ… Ø§Ù„Ù„ÙŠ Ù…Ø®Ù„ÙØ´ Ø¨Ù†Ø§ ÙˆÙ„Ø§ Ø§Ù„Ø§Ø® Ø§Ù„Ù„ÙŠ Ù…Ù„ÙˆØ´ Ø§Ø®Øª\n",
       "Name: Tweet, Length: 932, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.DataFrame(training_clean)\n",
    "training_cleaned['Valence score']= training_valence\n",
    "\n",
    "dev_cleaned = pd.DataFrame(dev_clean)\n",
    "dev_cleaned['Valence score']= dev_valence\n",
    "\n",
    "test_cleaned = pd.DataFrame(test_clean)\n",
    "test_cleaned['Valence score']= test_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.to_csv(training_outpath, sep=\"\\t\")\n",
    "dev_cleaned.to_csv(dev_outpath, sep=\"\\t\")\n",
    "test_cleaned.to_csv(test_outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Valence score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ø§Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ù… ÙŠØºÙ„Ø¨ ÙØ±Ø­</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ù…Ø§Ù Ø§Ù…Ø± Ø§Ù† ØºØ§Ø¶Ø¨ Ø§Ù†Ù†Ø§ Ù†Ø¸Ù‡Ø± Ù†Ù‚Ø¯Ù… Ø§Ø³Ø§Ø·ÙŠØ± Ø§Ù†ÙØ³ nØºØ§Ø¶Ø¨ Ø§Ù†ÙØ³ ØºØ±Ø¨</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ÙŠØ­Ø°Ø± ÙŠØ®ÙˆÙ Ø§Ø±ØªÙƒØ¨ Ø°Ù†Ø¨ Ù…Ø¹Øµ ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§Ø¨ ÙƒÙ†ÙˆØ² Ø¯Ø§Ø±Ø§Ù„Ø±ÙŠØ§Ù†Ø§Ù„Ù†Ø³Ø§Ø¡</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ØµØ¨Ø§Ø­ Ø³Ø¹Ø§Ø¯ Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ ØµÙŠØ§Ù… Ù‚ÙŠØ§Ù…</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ø´ÙØª Ø§Ø³Ø¨ÙˆØ¹ Ù…ØªØ´ÙˆÙ‚ ÙˆØ§ÙŠØ¯ ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ â¤ï¸</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>nÙ‡Ø° Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù† Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>ØµØ§Ø¯Ù… Ø­Ù‚ÙŠÙ‚ Ø§Ù…Ø± Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹ Ù…Ø´Ø§ÙƒØ³ ØªØ¶Ø§ÙŠÙ‚ Ù…Ø¬Ø¯Ø¯ ÙˆÙƒØ¡ Ù†ØªØ­Ø¯Ø« Ù‡Ø¯ÙˆØ¡ Ù‚Ù„ÙŠÙ„</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>Ø§ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø± Ø­Ù…Ù„ Ø±Ø§Ø³ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ ÙƒØªÙ ØªØ­Ø¨ ÙˆØ¬Ø¹</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>Ø¨Ø³Ù… Ø±Ø­Ù… Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠ Ø·Ø¹Ù… Ø±Ø¹Ø¨ ØµØ¯Ù‚ Ø³ÙˆÙ„Ù Ù…Ø¹ ÙˆØ­Ø¯ Ù†Ø§ÙŠÙ…Ù‡ğŸ˜­</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>ØªØ®Ø§Ù Ø§Ø®ÙˆØ§ Ø§ÙˆÙŠ Ø¯Ø§ÙŠÙ… Ø³Ø± Ø§Ø®Ùˆ ØªÙƒÙˆÙ† ÙØ±Ø­ Ø¨ÙŠØª Ù…Ø¨ÙŠØ¹Ø±ÙØ´ Ù‚ÙŠÙ… Ø§Ù„Ù„ÙŠ Ù…Ø®Ù„ÙØ´ Ø¨Ù†Ø§ ÙˆÙ„Ø§ Ø§Ù„Ø§Ø® Ø§Ù„Ù„ÙŠ Ù…Ù„ÙˆØ´ Ø§Ø®Øª</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>932 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        Tweet  \\\n",
       "0    Ø§Ù„Ù‰Ù° Ù…ØªÙ‰Ù° Ø§Ù„Ù… ÙŠØºÙ„Ø¨ ÙØ±Ø­                                                                     \n",
       "1    Ù…Ø§Ù Ø§Ù…Ø± Ø§Ù† ØºØ§Ø¶Ø¨ Ø§Ù†Ù†Ø§ Ù†Ø¸Ù‡Ø± Ù†Ù‚Ø¯Ù… Ø§Ø³Ø§Ø·ÙŠØ± Ø§Ù†ÙØ³ nØºØ§Ø¶Ø¨ Ø§Ù†ÙØ³ ØºØ±Ø¨                                  \n",
       "2    ÙŠØ­Ø°Ø± ÙŠØ®ÙˆÙ Ø§Ø±ØªÙƒØ¨ Ø°Ù†Ø¨ Ù…Ø¹Øµ ÙŠÙ†Ø²Ù„ Ø¨ÙƒÙ… Ø¹Ù‚Ø§Ø¨ ÙƒÙ†ÙˆØ² Ø¯Ø§Ø±Ø§Ù„Ø±ÙŠØ§Ù†Ø§Ù„Ù†Ø³Ø§Ø¡                                 \n",
       "3    ØµØ¨Ø§Ø­ Ø³Ø¹Ø§Ø¯ Ù…Ø¨Ø§Ø±Ùƒ ØªÙ‚Ø¨Ù„ ØµÙŠØ§Ù… Ù‚ÙŠØ§Ù…                                                             \n",
       "4    Ø´ÙØª Ø§Ø³Ø¨ÙˆØ¹ Ù…ØªØ´ÙˆÙ‚ ÙˆØ§ÙŠØ¯ ØµØ±Ø§Ø­Ù‡ ğŸ˜ğŸ˜ ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ â¤ï¸                                                \n",
       "..                                           ...                                                \n",
       "927  nÙ‡Ø° Ø­Ù„Ø§Ù„ ÙŠØ§Ø¨Ù† Ø­Ù„Ø§Ù„ğŸ‘ŒğŸ»ğŸ˜€ğŸ‰                                                                     \n",
       "928  ØµØ§Ø¯Ù… Ø­Ù‚ÙŠÙ‚ Ø§Ù…Ø± Ø®Ù„ÙˆØ¯ Ø±Ø¬Ø¹ Ù…Ø´Ø§ÙƒØ³ ØªØ¶Ø§ÙŠÙ‚ Ù…Ø¬Ø¯Ø¯ ÙˆÙƒØ¡ Ù†ØªØ­Ø¯Ø« Ù‡Ø¯ÙˆØ¡ Ù‚Ù„ÙŠÙ„                                \n",
       "929  Ø§ÙƒØªØ§Ù Ù‚Ø§Ø¯Ø± Ø­Ù…Ù„ Ø±Ø§Ø³ Ø³ÙˆØ¡ Ø­Ø¸Ùƒ ÙƒØªÙ ØªØ­Ø¨ ÙˆØ¬Ø¹                                                     \n",
       "930  Ø¨Ø³Ù… Ø±Ø­Ù… Ø±Ø­ÙŠÙ… Ø§Ø¹ÙˆØ° Ø¨Ø§Ù„Ù„Ù‡ Ù…Ø§Ø­Ø³ÙŠ Ø·Ø¹Ù… Ø±Ø¹Ø¨ ØµØ¯Ù‚ Ø³ÙˆÙ„Ù Ù…Ø¹ ÙˆØ­Ø¯ Ù†Ø§ÙŠÙ…Ù‡ğŸ˜­                               \n",
       "931  ØªØ®Ø§Ù Ø§Ø®ÙˆØ§ Ø§ÙˆÙŠ Ø¯Ø§ÙŠÙ… Ø³Ø± Ø§Ø®Ùˆ ØªÙƒÙˆÙ† ÙØ±Ø­ Ø¨ÙŠØª Ù…Ø¨ÙŠØ¹Ø±ÙØ´ Ù‚ÙŠÙ… Ø§Ù„Ù„ÙŠ Ù…Ø®Ù„ÙØ´ Ø¨Ù†Ø§ ÙˆÙ„Ø§ Ø§Ù„Ø§Ø® Ø§Ù„Ù„ÙŠ Ù…Ù„ÙˆØ´ Ø§Ø®Øª   \n",
       "\n",
       "    Valence score  \n",
       "0    -3            \n",
       "1    -2            \n",
       "2    -2            \n",
       "3    3             \n",
       "4    2             \n",
       "..  ..             \n",
       "927  1             \n",
       "928  -2            \n",
       "929  -1            \n",
       "930  -2            \n",
       "931  0             \n",
       "\n",
       "[932 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
