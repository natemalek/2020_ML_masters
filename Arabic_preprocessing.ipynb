{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Moved CleanText_Arabic function to preprocessing.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from snowballstemmer import stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"data/raw/2018-Valence-oc-Ar-train.txt\"\n",
    "dev_path = \"data/raw/2018-Valence-oc-Ar-dev.txt\"\n",
    "test_path = \"data/raw/2018-Valence-oc-Ar-test.txt\"\n",
    "\n",
    "training_outpath = \"data/cleaned/Ar_cleaned_training.txt\"\n",
    "dev_outpath = \"data/cleaned/Ar_cleaned_dev.txt\"\n",
    "test_outpath = \"data/cleaned/Ar_cleaned_test.txt\"\n",
    "\n",
    "stopwords_path = \"utilities/arabic-stop-words-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-Ar-01961</td>\n",
       "      <td>إلىٰ متىٰ الألم يغلب على الفرح</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-Ar-03289</td>\n",
       "      <td>@Al3mriRami @Holyliviuss كل مافي الأمر أني غاض...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-Ar-04349</td>\n",
       "      <td>يحذركم ويخوفكم من نفسه اذا ارتكبتم ذنب او معصي...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-Ar-03640</td>\n",
       "      <td>💞 💞 صباحكم سعادة في اليوم المبارك تقبل الله صي...</td>\n",
       "      <td>valence</td>\n",
       "      <td>3: very positive emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-Ar-01176</td>\n",
       "      <td>@sjalmulla شفته قبل اسبوع ومتشوقه عليه وايد ال...</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2018-Ar-01836</td>\n",
       "      <td>@GafnZx 😳\\n\\nهذا بالحلال يابنت  الحلال👌🏻😀🎉</td>\n",
       "      <td>valence</td>\n",
       "      <td>1: slightly positive emotional state can be in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2018-Ar-03475</td>\n",
       "      <td>الصادم في حقيقة الامر ان خلود رجعت مشاكسة مثل ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2018-Ar-01490</td>\n",
       "      <td>كل تلك الأكتاف قادرة على حمل رأسك لكن و ل سوء ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-1: slightly negative emotional state can be i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2018-Ar-01710</td>\n",
       "      <td>بسم الله الرحمن الرحيم اعوذ بالله ماحسيت بطعم ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-2: moderately negative emotional state can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2018-Ar-02662</td>\n",
       "      <td>بتخاف ع اخواتها اوى دايما سر اخوها بتكون فرحة ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  \\\n",
       "0    2018-Ar-01961                     إلىٰ متىٰ الألم يغلب على الفرح   \n",
       "1    2018-Ar-03289  @Al3mriRami @Holyliviuss كل مافي الأمر أني غاض...   \n",
       "2    2018-Ar-04349  يحذركم ويخوفكم من نفسه اذا ارتكبتم ذنب او معصي...   \n",
       "3    2018-Ar-03640  💞 💞 صباحكم سعادة في اليوم المبارك تقبل الله صي...   \n",
       "4    2018-Ar-01176  @sjalmulla شفته قبل اسبوع ومتشوقه عليه وايد ال...   \n",
       "..             ...                                                ...   \n",
       "927  2018-Ar-01836         @GafnZx 😳\\n\\nهذا بالحلال يابنت  الحلال👌🏻😀🎉   \n",
       "928  2018-Ar-03475  الصادم في حقيقة الامر ان خلود رجعت مشاكسة مثل ...   \n",
       "929  2018-Ar-01490  كل تلك الأكتاف قادرة على حمل رأسك لكن و ل سوء ...   \n",
       "930  2018-Ar-01710  بسم الله الرحمن الرحيم اعوذ بالله ماحسيت بطعم ...   \n",
       "931  2018-Ar-02662  بتخاف ع اخواتها اوى دايما سر اخوها بتكون فرحة ...   \n",
       "\n",
       "    Affect Dimension                                    Intensity Class  \n",
       "0            valence  -3: very negative emotional state can be inferred  \n",
       "1            valence  -2: moderately negative emotional state can be...  \n",
       "2            valence  -2: moderately negative emotional state can be...  \n",
       "3            valence   3: very positive emotional state can be inferred  \n",
       "4            valence  2: moderately positive emotional state can be ...  \n",
       "..               ...                                                ...  \n",
       "927          valence  1: slightly positive emotional state can be in...  \n",
       "928          valence  -2: moderately negative emotional state can be...  \n",
       "929          valence  -1: slightly negative emotional state can be i...  \n",
       "930          valence  -2: moderately negative emotional state can be...  \n",
       "931          valence  0: neutral or mixed emotional state can be inf...  \n",
       "\n",
       "[932 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_table(training_path)\n",
    "dev_df = pd.read_table(dev_path)\n",
    "test_df = pd.read_table(test_path)\n",
    "\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path, \"r\", encoding=\"utf-8\") as infile: \n",
    "    stopwords = list()\n",
    "    for line in infile:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_valence_scores(df): \n",
    "    valence_list = list()\n",
    "    for index, row in df.iterrows():\n",
    "        valence = row[\"Intensity Class\"]\n",
    "        valence = valence.replace(valence, valence[:2].replace(\":\", \"\"))\n",
    "        valence_list.append(valence)\n",
    "    return valence_list\n",
    "        \n",
    "training_valence = adapt_valence_scores(training_df)\n",
    "dev_valence = adapt_valence_scores(dev_df)\n",
    "test_valence = adapt_valence_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText_Arabic(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    From https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27\n",
    "    \"\"\"\n",
    "    def remove_repeating_char(self, input_text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', input_text) #keep 2 repeat\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r\"http\\S+ | www\\S+\" , \"لينك\", input_text)\n",
    "    \n",
    "    def remove_hashtags(self, input_text):\n",
    "        return re.sub(r\"#\", \"\", input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        ### whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords) and len(word) > 1] \n",
    "        return \" \".join(clean_words)\n",
    "    \n",
    "    #def stem(self, input_text):\n",
    "        #words = input_text.split()\n",
    "        #stemmed_words = [ar_stemmer.stemWord(word) for word in words]\n",
    "        #return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_hashtags).apply(self.remove_repeating_char).apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText_Arabic()\n",
    "training_clean = ct.fit_transform(training_df.Tweet)\n",
    "dev_clean = ct.fit_transform(dev_df.Tweet)\n",
    "test_clean = ct.fit_transform(test_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             إلىٰ متىٰ الألم يغلب الفرح\n",
       "1      مافي الأمر أني غاضب أننا نظهر نقدم أساطيرنا بأ...\n",
       "2      يحذركم ويخوفكم ارتكبتم ذنب معصيه سينزل بكم عقا...\n",
       "3               صباحكم سعادة المبارك تقبل صيامنا وقيامنا\n",
       "4      شفته اسبوع ومتشوقه وايد الصراحه 😍😍 بالتوفيق يا...\n",
       "                             ...                        \n",
       "927                        nهذا بالحلال يابنت الحلال👌🏻😀🎉\n",
       "928    الصادم حقيقة الامر خلود رجعت مشاكسة وتضايقني م...\n",
       "929         الأكتاف قادرة حمل رأسك سوء حظك الكتف تحب وجع\n",
       "930    بسم الرحمن الرحيم اعوذ بالله ماحسيت بطعم الرعب...\n",
       "931    بتخاف اخواتها اوى دايما سر اخوها بتكون فرحة ال...\n",
       "Name: Tweet, Length: 932, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.DataFrame(training_clean)\n",
    "training_cleaned['Valence score']= training_valence\n",
    "\n",
    "dev_cleaned = pd.DataFrame(dev_clean)\n",
    "dev_cleaned['Valence score']= dev_valence\n",
    "\n",
    "test_cleaned = pd.DataFrame(test_clean)\n",
    "test_cleaned['Valence score']= test_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.to_csv(training_outpath, sep=\"\\t\")\n",
    "dev_cleaned.to_csv(dev_outpath, sep=\"\\t\")\n",
    "test_cleaned.to_csv(test_outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
